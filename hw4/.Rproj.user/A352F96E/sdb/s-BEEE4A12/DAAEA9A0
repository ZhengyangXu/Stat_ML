{
    "contents" : "# Ethan Grant\n# uni: erg2145\n# class: STAT W4400\n# hw number: HW03\n\n#inputs:\n# x = training vectors\n# w = vector containing weights\n# y= vector of class lables\n#outputs:\n#list containing triplet (j, theta, m) that specifies the decision stump\nAdaBoost <- function(data, y, B){\n  w <- rep(0,dim(data)[1])\n  pars <- rep(list(list()), B)\n  alpha <- rep(0, dim(data)[1])\n  identity_funct <- rep(0, dim(data)[1])\n  \n  for(i in 1:dim(data)[1]){\n    w[i]<- 1/dim(data)[1]\n  }\n  \n  #train b classifiers while modifying weights\n  for(i in 1:B){\n    #train weak learners\n    pars[[i]] <- train(data, w, y)\n    \n    #find the labels\n    label <- classify(data,pars[[i]])\n    identity_funct <- (y!=label)\n    error <- (w %*% identity_funct/sum(w))[1]\n\n    #voting weights\n    alpha[i] = log((1-error)/error)\n    w <- w*exp(alpha[i]*identity_funct)\n  }\n  \n  return(list(pars = pars, alpha=alpha))\n  \n}\n\n#uses the aggregated pars to classify with aggregation\n#inputs:\n#x = data to be classified\n#alpha = alphas for data\n#pars = collection of weak learners\n#ouput: agg_class = aggregated classifications\nagg_class<- function(X, alpha, pars){\n  agg_classes <- matrix(0, dim(X)[1], length(alpha))\n  #computes classification without weights for each classifier\n  for(classifier in 1:length(alpha)){\n    indiv_class <- classify(X, pars[[classifier]])\n    agg_classes[,classifier] <- indiv_class\n  }\n  \n  #computes classfication with weights\n  agg_classes <- agg_classes %*% alpha\n  classes_signs <- sign(agg_classes)\n  \n  return(classes_signs=classes_signs)\n}\n\n#trains a weak learner\n#inputs:\n#x = data\n#w = weights\n#y = correct class\n#outputs:\n# pars the data that defines a weak learner\ntrain <- function(X, w, y){\n  \n  #initalize the things that define the stump\n  losses <- rep(0,dim(X)[2])\n  stump_split_spot <- rep(0,dim(X)[2])\n  m <- rep(0,dim(X)[2])\n\n  #compute splitting point for a given dim\n  for(i in 1:dim(X)[2]){\n    \n    #sort all dimensions into ascending order\n    sort_order <- order(X[,i])\n    x_sort <- X[sort_order,i]\n    \n    mistakes_for_each_split <- cumsum(w[sort_order]*y[sort_order])\n    \n    #find and mark duplicates as 0\n    duplicates <- duplicated(x_sort)\n    mistakes_for_each_split[duplicates] <- 0\n    \n    #find absolute max val note: 0's disregarded as can't be max\n    best_stump <- max(abs(mistakes_for_each_split))\n\n    #find the first index where the max val occurs\n    best_index <- min(which(abs(mistakes_for_each_split)==best_stump))\n   if(mistakes_for_each_split[best_index]<0){\n        m[i] <- 1\n    }\n    else{\n      m[i] <- -1\n    }\n    #find best split point\n    stump_split_spot[i] <- x_sort[best_index]\n\n\n    class <- rep(0, length(x_sort))\n    for(j in 1:length(class)){\n      if(x_sort[j]>stump_split_spot[i]){\n        class[j] <- m[i]\n      }\n      else{\n        class[j] <- -m[i]\n      }\n    }\n\n    losses[i] <- w %*% (class!=y)\n   \n  }\n  #best split point has lowest loss\n  best_split <- which.min(losses)\n  \n  pars <-list(best_split=best_split, stump_split= stump_split_spot[best_split], m=m[best_split])\n  return(pars)\n}\n\n#classifies X based on a single learner pars\n#inputs\n#x data\n#pars = weak learner\n#output\n#label = class labels\nclassify<-function(x,pars){\n  stump_split <- pars$stump_split\n  m <- pars$m\n  split <- pars$best_split\n  label <- rep(0, dim(x)[1])\n  \n  for(i in 1:dim(x)[1]){\n    if(x[i,split]>stump_split){\n      label[i] = m\n    }\n    else{\n      label[i] = -m\n    }\n  }\n\n  return(label)\n}\n\n#main functionf or adaboost\nmain <-function(){\n  data <- read.table(\"uspsdata.txt\")\n  class <- read.table(\"uspscl.txt\")\n  \n  test_index <- sample(1:dim(data)[1], dim(data)[1]/4)\n  \n  #divide data into test and trian\n  test_data <- data[test_index,]\n  test_class <- class[test_index,]\n  \n  train_data <- data[-test_index,]\n  train_class <- class[-test_index,]\n  \n  #create matrices to hold results from validating/testing learners\n  validation_err <- matrix(0, 50, 5) \n  test_err <- matrix(0, 50, 5)\n  \n  #5 fold cross validation\n  for(j in 1:5){\n    #split up into validation/train sets\n    validation_index <- ((j-1)*dim(test_data)[1]/5):((j)*dim(test_data)[1]/5)\n    new_train <- train_data[-validation_index,]\n    validation_data <- train_data[validation_index,]\n    new_class <- train_class[-validation_index]\n    validation_class <-train_class[validation_index]\n      \n    #run adaBoost and get 50 weak learners\n    result <- AdaBoost(new_train, new_class, 50)\n    pars <- result$pars\n    alpha <- result$alpha\n    print(\"learners trained!\")\n    \n    for(i in 1:50){\n      #get results for different numbers of aggregated weak learners on validation and compute/store error\n      result_class_validation <- agg_class(train_data, alpha[1:i], pars[1:i])\n      validation_err[i,j] <- mean(train_class!= result_class_validation)\n\n      #get results for different numbers of aggregated weak learners on test and compute/store error\n      result_class_test <- agg_class(test_data, alpha[1:i], pars[1:i])\n      test_err[i,j] <- mean(test_class!= result_class_test)\n    \n    }\n    \n  }\n  print(\"plotting!\")\n  matplot ( validation_err , type = \"l\" , lty =1: 5 , main = \"train err\" ,  xlab = \"weak learners\" , ylab = \"error rate on validation\" , ylim = c (0 ,0.75))\n  \n  matplot ( test_err , type = \"l\" , lty =1: 5 , main = \" test error \" , xlab = \"weak learners \" , ylab = \" error rate on test\" , ylim = c (0 ,0.575))\n  \n  \n}\n\nmain()\n\n",
    "created" : 1459963907883.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "887455670",
    "id" : "DAAEA9A0",
    "lastKnownWriteTime" : 1457586357,
    "path" : "C:/Users/Ethan/Downloads/hw3_prob1.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}