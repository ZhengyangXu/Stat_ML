rbind(norm_history, norm)
#check for mistakes and calc cost
mistakes <- (classifications<=0)
cost = sum(mistakes)
}
output <- list(norm=norm, norm_history=norm_history)
return(output)
}
print("starting")
z <- runif(4)
train_data <- fakedata(z, 100)
sample <- train_data$S
class <- train_data$y
percept <- perceptrain(sample, class)
norm <- percept$norm
classes <- classify(sample, norm)
tab <- table(class, classes)
tab
if(all(class==classes)){
print("works")
}
test_data <- fakedata(z, 100)
sample <- test_data$S
class <- test_data$y
classes <- classify(sample, norm)
tab <- table(class, classes)
tab
print("ending")
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
#inputs:
# S = Linearly seperaable data
# z = normal vector to hyperplane and offset
#outputs:
# classification = a vector containing classification for each point
classify <- function(S,z){
classification =  sign(S %*% z)
for(i in length(classification))
{
if(classification[i]==0)
{
classification[i]<-1
}
}
return(classification)
}
# inputs:
# S = sample of linearlly seperable data in a matrix
# y = classification for each data point
# outputs:
# norm = normal vector of the hyperplane
# norm_history = matrix with the history of the normal vector
perceptrain <- function(S,y){
#makes 4th column equal to class label
#S[y==-1,] <- -S[y==-1,]
print(S)
#pick random vector as starting line (since data is random any number 1 to n works)
norm <-  S[1,]
k <- 1
norm_history <- norm
classifications <- sign(S %*% norm)
print(classifications)
mistakes <- (classifications != y)
print(mistakes)
cost = sum(mistakes)
while(cost > 0){
if(cost==1){
norm = norm + 1/k*S[mistakes,]
}else{
norm = norm + 1/k*colSums(S[mistakes,])
}
k <- k+1
#evaluate new line and add to history
classifications <- sign(S %*% norm)
rbind(norm_history, norm)
#check for mistakes and calc cost
mistakes <- (classifications!=y)
cost = sum(mistakes)
}
output <- list(norm=norm, norm_history=norm_history)
return(output)
}
print("starting")
z <- runif(4)
train_data <- fakedata(z, 100)
sample <- train_data$S
class <- train_data$y
percept <- perceptrain(sample, class)
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
#inputs:
# S = Linearly seperaable data
# z = normal vector to hyperplane and offset
#outputs:
# classification = a vector containing classification for each point
classify <- function(S,z){
classification =  sign(S %*% z)
for(i in length(classification))
{
if(classification[i]==0)
{
classification[i]<-1
}
}
return(classification)
}
# inputs:
# S = sample of linearlly seperable data in a matrix
# y = classification for each data point
# outputs:
# norm = normal vector of the hyperplane
# norm_history = matrix with the history of the normal vector
perceptrain <- function(S,y){
#makes 4th column equal to class label
S[y==-1,] <- -S[y==-1,]
print(S)
#pick random vector as starting line (since data is random any number 1 to n works)
norm <-  S[1,]
k <- 1
norm_history <- norm
classifications <- S %*% norm
print(classifications)
mistakes <- (classifications <= 0)
print(mistakes)
cost = sum(mistakes)
while(cost > 0){
norm = norm + 1/k*colSums(S[mistakes,])
k <- k+1
#evaluate new line and add to history
classifications <- S %*% norm
rbind(norm_history, norm)
#check for mistakes and calc cost
mistakes <- (classifications<=0)
cost = sum(mistakes)
}
output <- list(norm=norm, norm_history=norm_history)
return(output)
}
print("starting")
z <- runif(4)
train_data <- fakedata(z, 100)
sample <- train_data$S
class <- train_data$y
percept <- perceptrain(sample, class)
norm <- percept$norm
classes <- classify(sample, norm)
tab <- table(class, classes)
tab
if(all(class==classes)){
print("works")
}
test_data <- fakedata(z, 100)
sample <- test_data$S
class <- test_data$y
classes <- classify(sample, norm)
tab <- table(class, classes)
tab
print("ending")
help("colSums")
help("sum")
sum("True", "false", "true")
sum(true, false, true")
sum(true, false, true)
sum("true", "false", "false")
sum(TRUE, FALSE)
#Inputs
#w:  w[1:d] is the normal vector of a hyperplane,
#    w[d+1] = -c is the negative offset parameter.
#n: sample size
#Outputs
#S: n by (d+1) sample matrix with last col 1
#y: vector of the associated class labels
fakedata <- function(w, n){
if(! require(MASS))
{
install.packages("MASS")
}
if(! require(mvtnorm))
{
install.packages("mvtnorm")
}
require(MASS)
require(mvtnorm)
# obtain dimension
d <- length(w)-1
# compute the offset vector and a Basis consisting of w and its nullspace
offset <- -w[length(w)] * w[1:d] / sum(w[1:d]^2)
Basis <- cbind(Null(w[1:d]), w[1:d])
# Create samples, correct for offset, and extend
# rmvnorm(n,mean,sigme) ~ generate n samples from N(0,I) distribution
S <- rmvnorm(n, mean=rep(0,d),sigma = diag(1,d)) %*%  t(Basis)
S <- S + matrix(rep(offset,n),n,d,byrow=T)
S <- cbind(S,1)
# compute the class assignments
y <- as.vector(sign(S %*% w))
# add corrective factors to points that lie on the hyperplane.
S[y==0,1:d] <- S[y==0,1:d] + runif(1,-0.5,0.5)*10^(-4)
y = as.vector(sign(S %*% w))
return(list(S=S, y=y))
} # end function fakedata
#inputs:
# S = Linearly seperaable data
# z = normal vector to hyperplane and offset
#outputs:
# classification = a vector containing classification for each point
classify <- function(S,z){
classification =  sign(S %*% z)
for(i in length(classification))
{
if(classification[i]==0)
{
classification[i]<-1
}
}
return(classification)
}
# inputs:
# S = sample of linearlly seperable data in a matrix
# y = classification for each data point
# outputs:
# norm = normal vector of the hyperplane
# norm_history = matrix with the history of the normal vector
perceptrain <- function(S,y){
#reverse sample for neg class
S[y==-1,4] <- -S[y==-1,4]
#pick random vector as starting line (since data is random any number 1 to n works)
norm <-  S[1,]
k <- 1
norm_history <- norm
classifications <- S %*% norm
#Find number of mistakes
mistakes <- (classifications <= 0)
cost = sum(mistakes)
while(cost > 0){
#note colSum doesn't work when mistakes is one-dim
if(cost==1){
norm = norm + 1/k*S[mistakes,]
}else{
norm = norm + 1/k*colSums(S[mistakes,])
}
k <- k+1
#evaluate new line and add to history
classifications <- S %*% norm
rbind(norm_history, norm)
#check for mistakes and calc cost
mistakes <- (classifications<=0)
cost = sum(mistakes)
}
output <- list(norm=norm, norm_history=norm_history)
return(output)
}
print("starting")
z <- runif(4)
train_data <- fakedata(z, 100)
sample <- train_data$S
class <- train_data$y
percept <- perceptrain(sample, class)
help("read.table")
data <- read.table("uspsdata.txt")
class <- read.table("uspscl.txt")
data
class
#Name: Ethan Grant
#uni: erg2145
#class: Stats 4400
#Hw2
data <- read.table("uspsdata.txt")
class <- read.table("uspscl.txt")
data
class
#Name: Ethan Grant
#uni: erg2145
#class: Stats 4400
#Hw2
data <- read.table("uspsdata.txt")
class <- read.table("uspscl.txt")
data
class
#Name: Ethan Grant
#uni: erg2145
#class: Stats 4400
#Hw2
data <- read.table("uspsdata.txt")
class <- read.table("uspscl.txt")
data
class
data <- read.table("uspsdata.txt")
data <- read.table("uspsdata.txt")
class <- read.table("uspscl.txt")
data <- read.table("uspsdata.txt")
data <- read.table("uspsdata.txt")
data <- read.table("uspsdata.txt")
data <- read.table("uspsdata.txt")
B <- matrix( c(2,4,3,1,5,7), nrow=3, ncol=2)
View(B)
order(B[,1])
order(B[1,])
B
B<- order(B[,1])
B
help("order")
B <- matrix( c(2,4,3,1,5,7), nrow=3, ncol=2)
index<- order(B[,1])
b_new <- B[index,1]
b_new
help("cumsum")
sort(B[,1])
B[,1]
help("duplicated")
test <- c(1, 5, 7, 9, 10)
help("sort")
order(test)
help("order")
order(test[])
temp <- order(test[])
temp
test
test <- c(1, 5, 7, 9, 10)
test
dex <- order(test)
dex <- order(test)
dex
test <- c(1, 5, 7, 9, 10)
test
order(test)
test
test <- c(1, 5, 7, 9, 10)
order(test[1])
test
B <- matrix( c(2,4,3,1,5,7), nrow=3, ncol=2)
index<- order(B[,1])
index
help("duplicated")
x <- c(-1, 2, 3, -4)
(x<0)
(x<0)*2
(x<0)*2-1
x> 1
t <- H[sample(dim(H)[1], K),]
for(i in 1:K){
t[i,] = normalize.vector(t[i,])
}
#set up variables
phi = matrix(, dim(H)[1], K)
a = matrix(, dim(H)[1], K)
c <- rep(1/3, 3)
#calculate every phi value
for(k in 1:K){
sumVec <- H %*% log(t[k,])
for(i in 1: dim(H)[1]){
phi[i,k] = exp(sumVec[i])
}
}
#compute posterior probability
for(i in 1:dim(H)[1]){
for(k in 1:K){
sum <- sum(c*phi[i,])
a[i,k] <- c[k]*phi[i,k]/sum
}
}
K=3
t <- H[sample(dim(H)[1], K),]
for(i in 1:K){
t[i,] = normalize.vector(t[i,])
}
#set up variables
phi = matrix(, dim(H)[1], K)
a = matrix(, dim(H)[1], K)
c <- rep(1/3, 3)
#calculate every phi value
for(k in 1:K){
sumVec <- H %*% log(t[k,])
for(i in 1: dim(H)[1]){
phi[i,k] = exp(sumVec[i])
}
}
#compute posterior probability
for(i in 1:dim(H)[1]){
for(k in 1:K){
sum <- sum(c*phi[i,])
a[i,k] <- c[k]*phi[i,k]/sum
}
}
H <- matrix(readBin("histograms.bin", "double", 640000), 40000, 16)
setwd("C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4")
H <- matrix(readBin("histograms.bin", "double", 640000), 40000, 16)
source('C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4/hw_4.R', echo=TRUE)
source('C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4/hw_4.R', echo=TRUE)
source('C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4/hw_4.R', echo=TRUE)
source('C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4/hw_4.R', echo=TRUE)
K=3
t <- H[sample(dim(H)[1], K),]
for(i in 1:K){
t[i,] = normalize.vector(t[i,])
}
#set up variables
phi = matrix(, dim(H)[1], K)
a = matrix(, dim(H)[1], K)
c <- rep(1/3, 3)
#calculate every phi value
for(k in 1:K){
sumVec <- H %*% log(t[k,])
for(i in 1: dim(H)[1]){
phi[i,k] = exp(sumVec[i])
}
}
#compute posterior probability
for(i in 1:dim(H)[1]){
for(k in 1:K){
sum <- sum(c*phi[i,])
a[i,k] <- c[k]*phi[i,k]/sum
}
}
source('C:/Users/Ethan/Desktop/Columbia/Junior Spring/Stat ML/hw4/hw_4.R', echo=TRUE)
H <- H+.001
K=3
t <- H[sample(dim(H)[1], K),]
for(i in 1:K){
t[i,] = normalize.vector(t[i,])
}
#set up variables
phi = matrix(, dim(H)[1], K)
a = matrix(, dim(H)[1], K)
c <- rep(1/3, 3)
#calculate every phi value
for(k in 1:K){
sumVec <- H %*% log(t[k,])
for(i in 1: dim(H)[1]){
phi[i,k] = exp(sumVec[i])
}
}
#compute posterior probability
for(i in 1:dim(H)[1]){
for(k in 1:K){
sum <- sum(c*phi[i,])
a[i,k] <- c[k]*phi[i,k]/sum
}
}
for(k in 1:K){
c <- colSums(a)
b <- a[,k]%*%H
}
for(k in 1:K){
c <- colSums(a)
b <- a[,k]*H
}
